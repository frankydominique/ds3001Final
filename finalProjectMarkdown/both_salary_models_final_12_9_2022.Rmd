---
title: "Final3"
author: "Claire Yoon (ndq7xj)"
date: "2022-12-07"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rattle)
library(caret)
library(C50)
library(readr)
library(mlbench)
library(corrplot)
library(ggcorrplot)
library(NbClust)
library(data.table)
library(scales)
```


```{r}
# read the STEM data
data <- read.csv("Levels_Fyi_Salary_Data.csv")
str(data)
# check the dimension of the data
dim(data)
# check the column index
column_index <- tibble(colnames(data))
# drop unneeded variables
data2 <- data[, c(-1,-3,-9,-14,-15,-16,-17,-18,-19,-20,-21,-22,-23,-24,-25,-26,-27,-28)]
# get rid of weird value in gender variable (Title: Senior Software Engineer)
data2 <- data2[-11011, ]
table(data2$gender)
# analyze each variable
table(data2$company)
table(data2$level)
table(data2$title) # no missing data in this variable
table(data2$location)
table(data2$gender)
table(data2$Education)
# dealing with the missing data
data3 <- data2[complete.cases(data2), ]
dim(data3)
# option 1 to factorize title
data3$title <- fct_collapse(data3$title, 
                           BUSI.MGMT = c("Business Analyst", "Management Consultant", "Marketing", "Product Manager", "Sales", "Technical Program Manager"),
                           Engineer.Scientist = c("Data Scientist", "Hardware Engineer", "Mechanical Engineer", "Software Engineer","Software Engineering Manager"),
                           Other = c("Human Resources", "Product Designer", "Recruiter", "Solution Architect")
                          )
# option 2 to factorize title
#data2$title <- fct_collapse(data2$title,
#                            Engineer = c("Hardware Engineer", "Mechanical Engineer", "Software Engineer", "Software #Engineering Manager"),
#                            Other = c("Business Analyst", "Management Consultant", "Marketing", "Product Manager", #"Sales", "Technical Program Manager", "Human Resources", "Product Designer", "Recruiter", "Solution Architect", "Data #Scientist")
#                            )
table(data3$title)
str(data3$company)
#this gets all the column value in companies except for those
FAANG = data3 %>% filter(company %in% c("Facebook", "Amazon", "Apple", "Netflix", "Google", "AWS", "google", "Google LLC", "GOogle"))
Tier2 = data3 %>% filter(company %in% c("Microsoft", "IBM", "Capital One", "Capital one", "Ibm", "ibm", "Intel", "intel", "Intel Corporation", "intel corporation", "Intel corporation", "jp morgan", "Jp Morgan", "JP Morgan", "JP Morgan Chase", "JPMorgan", "JPMORGAN", "JPmorgan Chase", "JPMorgan Chase", "microsoft", "MICROSOFT", "microsoft corporation", "Microsoft Corporation", "Oracle", "oracle", "ORACLE", "paypal", "Paypal", "PayPal", "Salesforce", "salesforce", "SAP", "Sap", "SAP Concur", "Shopify", "Accenture", "Adobe", "Bloomberg", "Bloomberg LP", "Cisco", "Cisco Meraki", "cisco", "cisco systems", "Cisco Systems", "CISCO SYSTEMS", "Deloitte", "Deloitte Advisory", "Deloitte consulting", "Deloitte Consulting", "Deloitte Consulting LLP", "eBay", "ebay", "Ebay", "Expedia", "Expedia Group", "Goldman Sachs", "LinkedIn", "Linkedin", "Dell", "Dell Technologies", "Intuit", "Lyft", "Nvidia", "NVIDIA", "nvidia", "Qualcomm", "qualcomm", "ServiceNow", "Servicenow", "Twitter", "Uber", "UBER", "uber", "visa", "Visa", "VISA", "Visa inc", "Visa Inc", "vmware", "Vmware", "VMware", "VMWare", "walmart", "Walmart", "walmart labs", "Walmart labs", "walmart labs", "Wayfair"))
others = data3 %>% filter(!company %in% c("Facebook", "Amazon", "Apple", "Netflix", "Google", "AWS","Microsoft", "IBM", "Capital One", "Capital one", "Ibm", "ibm", "Intel", "intel", "Intel Corporation", "intel corporation", "Intel corporation", "jp morgan", "Jp Morgan", "JP Morgan", "JP Morgan Chase", "JPMorgan", "JPMORGAN", "JPmorgan Chase", "JPMorgan Chase", "microsoft", "MICROSOFT", "microsoft corporation", "Microsoft Corporation", "Oracle", "oracle", "ORACLE", "paypal", "Paypal", "PayPal", "Salesforce", "salesforce", "SAP", "Sap", "SAP Concur", "Shopify", "Accenture", "Adobe", "Bloomberg", "Bloomberg LP", "Cisco", "Cisco Meraki", "cisco", "cisco systems", "Cisco Systems", "CISCO SYSTEMS", "Deloitte", "Deloitte Advisory", "Deloitte consulting", "Deloitte Consulting", "Deloitte Consulting LLP", "eBay", "ebay", "Ebay", "Expedia", "Expedia Group", "google", "Google LLC", "GOogle", "Goldman Sachs", "LinkedIn", "Linkedin", "Dell", "Dell Technologies", "Intuit", "Lyft", "Nvidia", "NVIDIA", "nvidia", "Qualcomm", "qualcomm", "ServiceNow", "Servicenow", "Twitter", "Uber", "UBER", "uber", "visa", "Visa", "VISA", "Visa inc", "Visa Inc", "vmware", "Vmware", "VMware", "VMWare", "walmart", "Walmart", "walmart labs", "Walmart labs", "walmart labs", "Wayfair"))
vec1 = pull(FAANG, company)
vec2 = pull(Tier2, company)
vec3 = pull(others, company)
data3$company <- fct_collapse(data3$company, 
                           FAANG = vec1,
                           Tier2 = vec2,
                           Other = vec3
                          )
table(data3$company)
table(data3$Education)
data3$Education <- fct_collapse(data3$Education,
                            Bachelor = "Bachelor's Degree",
                            Master = "Master's Degree",
                            PhD = "PhD",
                            Other = c("Highschool", "Some College")
                            )
table(data3$Education)
str(data3)
# convert to factors
data3[,c(4, 10)] <- lapply(data3[,c(4, 10)], as.factor)
str(data3)
```


```{r}
# Check for missing variables and correct as needed.  
mice::md.pattern(data3) # good
```

```{r}
# Normalize data
#normalize = function(x){
#  (x - min(x)) / (max(x) - min(x))
# }
# 
# numerics = names(select_if(data3, is.numeric))
# data3[numerics] = lapply(data3[numerics], normalize)
# str(data3)
```


## Splitting the Data: 
```{r}
set.seed(777)
partition <- caret::createDataPartition(data3$totalyearlycompensation,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- data3[partition, ]
tune_and_test <- data3[-partition, ]
train
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$totalyearlycompensation,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test) 
dim(tune)
```


```{r}
#5 Build your model using the training data, rpart2, and repeated cross validation as reviewed in class with the caret package.
# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.
str(data3)
features <- train[,-3] # dropping the target variable (totalyearlycompensation). 
#View(features)
target <- train$totalyearlycompensation
str(features)
str(target)
#Three steps in building a caret ML model
#Step 1: Cross validation process-the process by which the training data will be used to build the initial model must be set. As seen below:
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5) 
# number - number of folds
# repeats - number of times the CV is repeated, takes the average of these repeat rounds
#review the documentation on https://topepo.github.io/caret/measuring-performance.htm
#Step 2: Usually involves setting a hyper-parameter search. This is optional and the hyper-parameters vary by model. Let's take a look at the documentation for the model we are going to use. Same search function as for classification 
tree.grid <- expand.grid(maxdepth=c(3:20))
#Step 3: Train the model
set.seed(777)
salary_mdl <- train(x=features,
                y=target,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="RMSE")
str(target)
```


```{r}
# View the results, comment on how the model performed and which variables appear to be contributing the most (variable importance)  
salary_mdl
salary_mdl$finalModel
plot(salary_mdl)
varImp(salary_mdl)
# check what variables contributed to the target variable!
salary_mdl$finalModel$variable.importance
salary_mdl$results
# considering the importance of variable and results(decision tree), the most contributing variable is 'basesalary'.
```

```{r}
# our model
rpart.plot(salary_mdl$finalModel, type=4, extra=101)
```


```{r}
# predicting using base model and test set
pred_test_reg <- predict(salary_mdl,test)
postResample(pred = pred_test_reg,obs = test$totalyearlycompensation)
comparison <- data.frame(test['totalyearlycompensation'], pred_test_reg)
View(comparison)
```


```{r}
# Normalize data
normalize = function(x){
 (x - min(x)) / (max(x) - min(x))
}
numerics = names(select_if(comparison, is.numeric))
comparison2 = lapply(comparison[numerics], normalize)
View(comparison2)
# Creating overlapping density plots
plot(density(comparison2$totalyearlycompensation), col = "blue", main = "Density plots of actual and predicted compensation", xlab = "Normalized compensation", ylab = "Density")
lines(density(comparison2$pred_test_reg), col = "red")
legend("topright", legend=c("Actual", "Predicted"), 
       fill = c("blue","red")
)
```

```{r}
RMSE = 59478
calc_range = range(tune$totalyearlycompensation)
calc_range
low = calc_range[1]
high = calc_range[2]
range = high - low
NRMSE = RMSE/range
NRMSE
# NRMSE = 0.03740755
```


```{r}
ggplot(comparison,                                     
       aes(x = pred_test_reg,
           y = totalyearlycompensation)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)+
  ggtitle("Actual compensation vs. Predicted compensation")+
  labs(y = "Actual yearly compensation", x = "Predicted yearly compensation")+
  scale_x_continuous(labels = comma)+
  scale_y_continuous(labels = comma)
```


##Real world model (we don't know base salary, stock, and bonus)
```{r}
data4 <- data3[, c(-7,-8,-9)]
View(data4)

set.seed(777)
partition2 <- caret::createDataPartition(data4$totalyearlycompensation,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train2 <- data4[partition2, ]
tune_and_test2 <- data4[-partition2, ]
train2
#The we need to use the function again to create the tuning set 
tune_and_test_index2 <- createDataPartition(tune_and_test2$totalyearlycompensation,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune2 <- tune_and_test2[tune_and_test_index2, ]
test2 <- tune_and_test2[-tune_and_test_index2, ]
dim(train2)
dim(test2) 
dim(tune2)

str(data4)
features2 <- train2[,-3] # dropping the target variable (totalyearlycompensation). 
#View(features)
target2 <- train2$totalyearlycompensation
str(features2)
str(target2)

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5) 

tree.grid <- expand.grid(maxdepth=c(3:20))
#Step 3: Train the model
set.seed(777)
salary_mdl2 <- train(x=features2,
                y=target2,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="RMSE")
str(target2)

```

```{r}
# View the results, comment on how the model performed and which variables appear to be contributing the most (variable importance)  
salary_mdl2
salary_mdl2$finalModel
plot(salary_mdl2)
varImp(salary_mdl2)
# check what variables contributed to the target variable!
#years of experience most important

salary_mdl2$finalModel$variable.importance

salary_mdl2$results
# considering the importance of variable and results(decision tree), the most contributing variable is 'location'.
```

```{r}
# our model
rpart.plot(salary_mdl2$finalModel, type=4, extra=101)

```


```{r}
# predicting using base model and test set
pred_test_reg2 <- predict(salary_mdl2,test2)
postResample(pred = pred_test_reg2,obs = test2$totalyearlycompensation)
comparison3 <- data.frame(test2['totalyearlycompensation'], pred_test_reg2)
View(comparison3)
```


```{r}
# Normalize data

numerics2 = names(select_if(comparison3, is.numeric))
comparison4 = lapply(comparison3[numerics2], normalize)
View(comparison4)
# Creating overlapping density plots
plot(density(comparison4$totalyearlycompensation), col = "blue", main = "Density plots of actual and predicted compensation", xlab = "Normalized compensation", ylab = "Density")
lines(density(comparison4$pred_test_reg2), col = "red")
legend("topright", legend=c("Actual", "Predicted"), 
       fill = c("blue","red")
)
```

```{r}
RMSE = 100492
calc_range = range(tune2$totalyearlycompensation)
calc_range
low = calc_range[1]
high = calc_range[2]
range = high - low
NRMSE = RMSE/range
NRMSE
# NRMSE = 0.06320252
```


```{r}
ggplot(comparison,                                     
       aes(x = pred_test_reg2,
           y = totalyearlycompensation)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)+
  ggtitle("Actual compensation vs. Predicted compensation")+
  labs(y = "Actual yearly compensation", x = "Predicted yearly compensation")+
  scale_x_continuous(labels = comma)+
  scale_y_continuous(labels = comma)
```

